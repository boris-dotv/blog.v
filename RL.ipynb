{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "强化学习的几个组成部分:  \n",
    "* **Agent**, agents learn to take actions to maximize expected reward.\n",
    "* **Action**, change the environment.\n",
    "* **Environment**, the place where agents are expected to maximize expected reward.\n",
    "* **Reward**, a feedback from pre-defined rule or reward model.\n",
    "* **Observation**, agents 对当前环境 state 的观察, 注意 state 不总是等于 observation, 比如在象棋游戏中, 当前棋盘的 state 就是 agents 的 observation, 在扑克牌中, 当前棋盘的 state 就是打出的牌和所有 agents 手里的牌, 但 observation 是 agents 手里的牌. state 是\"上帝视角\", observation 是 agent 视角.\n",
    "\n",
    "### Policy-based Approach (Learning an Actor)\n",
    "Machine learning 的终极目标可以大致描述为 Looking for a function. 在 RL 中, Observation, Actor/Policy, Action 的关系可以表示为:  \n",
    "\n",
    "$$Action = \\pi(Observation)$$\n",
    "\n",
    "我们定义 $\\bar{R}_{\\theta}$ 作为 $R_{\\theta}$ 的期望值.\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "$J$ 在此处是 Objective Function, PPO 的目标是:\n",
    "$$\\underset{\\theta}{\\text{maximize}} \\ J_{\\mathrm{PPO}}^{\\theta'}(\\theta) = J^{\\theta'}(\\theta) - \\beta \\, \\mathrm{KL}(\\theta, \\theta')$$\n",
    "其中:\n",
    "$$J^{\\theta'}(\\theta) = \\mathbb{E}_{(s_t, a_t) \\sim \\pi_{\\theta'}} \\left[ \\frac{p_{\\theta}(a_t \\mid s_t)}{p_{\\theta'}(a_t \\mid s_t)} A^{\\theta'}(s_t, a_t) \\right]$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
